---
title: "BDA - Project"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 1
urlcolor: blue
---


```{r message=FALSE, warning=FALSE}
library(aaltobda)
library(tidyverse)
library(rstan)
library(rstanarm)
options(mc.cores=parallel::detectCores()) # enable parallelism
rstan_options(auto_write=TRUE)            # whatever some warning told me to do
```

```{r}
# set seed for reproducibility
SEED <- 687542
set.seed(SEED)

# load dataset
data <- read.csv("dataset.csv")
data <- subset(data, select=-c(time))

# split into train and test data
train_size <- floor(0.8 * nrow(data))
index <- sample(seq_len(nrow(data)), size=train_size)
data.train <- data[index, ]
data.test <- data[-index, ]
```

# Introduction

Cardiovascular disease is one of the most common death causes in the world. It kills around 17 million people annually. Most common death cause for the cardiovascular patient is Heart failure which is a situation in which the heart cannot pump enough blood compared to the amount the body needs to be functional. As there is only a very limited time between the beginning of heart failure and death it is critical to identify which part of the population is in the risk group for it and which factors predict likelihood of the heart failure.

The problem which this project is trying to solve is, how to predict whether heart failure patient will survive over the follow up period. This is essential in order to adjust the follow up times such that patient is alive to participate follow up meeting with the doctor after follow up period.

The idea is to create a model which predicts heart failure patients' death during the follow up period. In this report we use one hierarcical and one non-hierarcical model to this task. Both models are linear and they use logit function as link function as the target predict binary outcome of whether or not the patient dies during follow up period.


# Dataset

Used data contain medical records of 299 heart failure patients. The records are collected during April-December 2015 from Faisalabad Institute of Cardiology and at the Allied Hospital in Faisalabad(Punjab, Pakistan). There are 105 female and 194 male patients in the data. They are from the age range between 40 and 95. 

In dataset there are 13 attributes recorded about every patients:

- age: in year

- anaemia: boolean value  which describe whether or not there are decrease of blood cells or hemoglobin

- High blood pressure: boolean value which indicate hypertension

- creatinine phosphokinase (CPK): level of the CPK enzyme in the blood in mcg/L

- diabetes: boolean value about whether the patient has diabetes (boolean)

- ejection fraction: percentage of blood leaving the heart at each contraction (percentage)

- platelets: platelets in the blood in kiloplatelets/mL

- sex: boolean value which indicate gender such that female have value zero and male value one

- serum creatinine: level of serum creatinine in the blood (mg/dL)

- serum sodium: level of serum sodium in the blood (mEq/L)

- smoking: Boolean value about whether the patient smokes or not 

- time: follow-up period in days

- death event: Boolean value about the patient deceased during the follow-up period

In the models of this report death, the death event is the attribute which is tried to predict using other features. Creatine phosphokinase and, time and platelet are filtered out as they do not have remarkable influence to the outcome.

# Weaky-informative hierarchical model
Firstly, we divided the samples to two groups according to gender. For each group, we build a logistic classification. In the ith group, the death event $y_{ij}$ follows a bernoulli distribution whose probability is defined by a logistic function on independent variables.
$$Likelihood: y_{ij}\sim bernoulli(logit^{-1}(\beta_{i} \times x_{ij})),\\ where\  y_{ij}\  is\ binary,\   x_{ij}\  represents\  feature\ vector,\ and\ \beta_i\ is\  coefficient\ vector. $$
$$Prior: \beta_{ik} \sim Normal(hyper\_mu_{k},hyper\_sigma_{k})$$

In this case we used weakly-informative priors for hypermu. Since each group is fitted with a logistic model and $\beta \times x =0$ is the hyperplane of classifier, coefficient $\beta_i$ is the slope of hyperplane in corresponding dimension. 
It is very unlikely that any coefficient could surpass 25. Thus, we use prior for hyper parameter as following: 

$$ Hyper\ prior: hyper\_mu \sim Normal(0,10)$$



```{stan output.var="m1"}
// hierarchical model
// likelihood: bernoulli_logit
// prior: normal
// hyper prior: normal

data {
  int<lower=0> N;            // number of observations
  int<lower=1> M;            // number of data groups
  int<lower=1> K;            // number of predictors
  int<lower=0,upper=1> y[N]; // outcome (binary)
  int<lower=1,upper=M> g[N]; // map observations to groups
  row_vector[K] x[N];        // predictors
  // test data below
  int<lower=0> N_test;
  int<lower=1,upper=M> g_test[N_test];
  row_vector[K] x_test[N_test];
}
parameters {
  real mu[K];
  real<lower=0> sigma[K];
  vector[K] beta[M];
}
model {
  mu ~ normal(0, 10);
  for (m in 1:M)
    beta[m] ~ normal(mu, sigma);
  for (n in 1:N)
    y[n] ~ bernoulli_logit(x[n] * beta[g[n]]);
}
generated quantities {
  real y_test[N_test];
  real y_check[N];
  vector[N] log_lik;
  
  
  for (n in 1:N_test)
    y_test[n] = bernoulli_logit_rng(x_test[n] * beta[g_test[n]]);
  
  for (i in 1:N)
  {
    y_check[i]=bernoulli_logit_rng(x[i] * beta[g[i]]);
    log_lik[i] = bernoulli_logit_lpmf(y[i] | x[i] * beta[g[i]]);
  }
}
```

Before feature selection, we trained the model using all features and the accuracy as shown below is around 0.73.
```{r,message=FALSE,results='hide'}
#all features
m1.data <- list(
  N = nrow(data.train),       # number of observations
  M = 2,                      # number of groups (male / female)
  K = ncol(data.train) - 2,   # all but outcome + group (= sex)
  y = data.train$DEATH_EVENT,
  g = data.train$sex + 1,
  x = subset(data.train, select=-c(DEATH_EVENT, sex)),
  # test data
  N_test = nrow(data.test),
  g_test = data.test$sex + 1,
  x_test = subset(data.test, select=-c(DEATH_EVENT, sex))
)

m1.fit <- rstan::sampling(m1, data=m1.data, seed=SEED)
```

```{r}
m1.draws <- extract(m1.fit)
m1.preds <- as.integer((m1.draws$y_test %>% colMeans) >= 0.5)
m1.accuracy <- mean(m1.preds == data.test$DEATH_EVENT)
print(m1.accuracy)
```

## Feature selection

Using all features, the classification accuracy is not satisfied and some parameters show high R-hat. Thus, we conducted feature selection by deleting the features and combination of these features that are less relevant to 'DEATH_Event' in the correlation heat map one by one. Below table shows the accurate and largest R-hat corresponding to variable set. Using features except for 'platelets' and 'creatinine_phosphokinase' gives highest accuracy and lowest R-hat.
```{r}
library(corrplot)
cor_data <- cor(subset(data.train, select=-c(sex)),method="spearman")
corrplot(cor_data,method='square')
```

```{r}
param_selection <- data.frame(
    deleted_feature=c("none", "creatinine_phosphokinase", "serum_creatinine","platelets","diabetes","smoking","platelets,serum_creatinine","creatinine_phosphokinase,serum_creatinine","platelets,creatinine_phosphokinase","platelets,creatinine_phosphokinase,serum_creatinine"), 
    accuracy=c(0.733,0.616, 0.733, 0.75,0.65,0.7,0.7,0.766,0.783,0.733), 
    largest_R_hat=c(4.17,4.13,4.34 , 1.85,'infinite',13.82,1.33,4.22,1.09,1.11),
    stringsAsFactors=FALSE)
knitr::kable(param_selection,align = "lll")

```
Below shows the performance when deleting 'platelets' and 'creatinine_phosphokinase'.
```{r,message=FALSE,results='hide'}
#selected features
m2.data <- list(
  N = nrow(data.train),       # number of observations
  M = 2,                      # number of groups (male / female)
  K = ncol(data.train) - 4,   # all but outcome + group (= sex)
  y = data.train$DEATH_EVENT,
  g = data.train$sex + 1,
  x = subset(data.train, select=-c(DEATH_EVENT, sex,platelets,creatinine_phosphokinase)),
  # test data
  N_test = nrow(data.test),
  g_test = data.test$sex + 1,
  x_test = subset(data.test,select=-c(DEATH_EVENT,sex,platelets,creatinine_phosphokinase))
)

m2.fit <- rstan::sampling(m1, data=m2.data, seed=SEED)
```

```{r}
m2.draws <- extract(m2.fit)
m2.preds <- as.integer((m2.draws$y_test %>% colMeans) >= 0.5)
m2.accuracy <- mean(m2.preds == data.test$DEATH_EVENT)
print(m2.accuracy)
```

# Pooled model
The pooled model trains a logistic classification model for all samples regardless of gender.
$$Likelihood: y_{j}\sim bernoulli(logit^{-1}(\beta \times x_{j})) $$
$$Prior: \beta_{k} \sim Normal(0,40)$$

In this case we used weakly-informative priors for $\beta$. Simialr as hierarchical model,P(<-25$\beta$<25)>0.99. As shown below, accuracy of this model is 0.75.

```{stan output.var="pool"}
data {
  int<lower=0> N;            // number of observations
  int<lower=1> K;            // number of predictors
  int<lower=0,upper=1> y[N]; // outcome (binary)
  row_vector[K] x[N];        // predictors
  // test data below
  int<lower=0> N_test;
  row_vector[K] x_test[N_test];
}
parameters {
  vector[K] beta;
}
model {
  beta ~ normal(0, 10);
  for (n in 1:N)
    y[n] ~ bernoulli_logit(x[n] * beta);
}
generated quantities {
  real y_test[N_test];
  real y_check[N];
  vector[N] log_lik;
  
  for (i in 1:N){
    log_lik[i] = bernoulli_logit_lpmf(y[i] | x[i] * beta);
    y_check[i]=bernoulli_logit_rng(x[i] * beta);
  }
    
  for (n in 1:N_test)
    y_test[n] = bernoulli_logit_rng(x_test[n] * beta);
}
```

```{r,message=FALSE,results='hide'}
#selected features
pool.data <- list(
  N = nrow(data.train),       # number of observations
  K = ncol(data.train) - 4,   # all but outcome + group (= sex)
  y = data.train$DEATH_EVENT,
  x = subset(data.train, select=-c(DEATH_EVENT, sex,platelets,creatinine_phosphokinase)),
  # test data
  N_test = nrow(data.test),
  x_test = subset(data.test,select=-c(DEATH_EVENT,sex,platelets,creatinine_phosphokinase))
)

pool.fit <- rstan::sampling(pool, data=pool.data, seed=SEED)
```

```{r}
pool.draws <- extract(pool.fit)
pool.preds <- as.integer((pool.draws$y_test %>% colMeans) >= 0.5)
pool.accuracy <- mean(pool.preds == data.test$DEATH_EVENT)
print(pool.accuracy)
```

# Convergence analysis
## R-hat convergence analysis

R-hat convergence diagnostic compares the between- and within-chain estimates for model parameters. If chains have not mixed well, R-hat is larger than 1.

R-hat value of hierarchical model is shown as following. For most parameters R-hat value is under 1.01, which means the simulation converges.
```{r}
rhat<-c()
for (i in 1:8)
{
  r_mu=Rhat(m2.draws$mu[,i])
  r_sigma=Rhat(m2.draws$sigma[,i])
  r_beta_1=Rhat(m2.draws$beta[,1,i])
  r_beta_2=Rhat(m2.draws$beta[,2,i])
  rhat<-c(rhat,c(r_mu,r_sigma,r_beta_1,r_beta_2))
}
print(rhat)
```


R-hat value of pooled model is shown as following. R-hat value for all parameters is under 1.001, which means the simulation from poole dmodel also converges.
```{r}
rhat<-c()
for (i in 1:8)
{
  
  r=Rhat(pool.draws$beta[,i])
  rhat<-c(rhat,r)
}
print(rhat)
```



## Effective sample size diagnostic

Effective sample size indicates efficiency in sampling. ESS values for most of  hierarchical model parameters are high, except for two parameters whose ESS value is only 250 and 15.
```{r}
ess_hier<-c()
for (i in 1:8)
{
  e_mu=ess_tail(m2.draws$mu[,i])
  e_sigma=ess_tail(m2.draws$sigma[,i])
  e_beta_1=ess_tail(m2.draws$beta[,1,i])
  e_beta_2=ess_tail(m2.draws$beta[,2,i])
  ess_hier<-c(ess_hier,c(e_mu,e_sigma,e_beta_1,e_beta_2))
}
print(ess_hier)
```

ESS value for all pooled model parameters are high, which means most simulation after warm-up is effective.
```{r}
ess_pool<-c()
for (i in 1:8)
{
  e=ess_tail(pool.draws$beta[,i])
  ess_pool<-c(ess_pool,e)
}
print(ess_pool)
```



# Posterior predictive check

Since predictive target is binary in this case, we chose the proportion of positive prediction as test quantity. The test quantity of real data is around 0.33. Following figure shows the distribution of test quantity for 4000 replicates from hierarchical model, whose interval contains 0.33. Thus, we could believe our model is reliable.
```{r}
ratio_true=sum(data.train$DEATH_EVENT)/length(data.train$DEATH_EVENT)
ratios<-c()
for (i in 1:4000){
  ratio=sum(m2.draws$y_check[i,] )/length(m2.draws$y_check[i,] )
  ratios<-c(ratios,ratio)
}
hist(ratios)
abline( v = ratio_true, col = "red")

```
Below figure show the posterior check result of pooled model. We could also believe pooled model is reliable.
```{r}
ratio_true=sum(data.train$DEATH_EVENT)/length(data.train$DEATH_EVENT)
ratios<-c()
for (i in 1:4000){
  ratio=sum(pool.draws$y_check[i,] )/length(pool.draws$y_check[i,] )
  ratios<-c(ratios,ratio)
}
hist(ratios)
abline( v = ratio_true, col = "red")

```
# Model Comparison
PSIS-LOO elpd value of hierarchical model is around -130, while for pooled model it's -125. According to the comparison, pooled model is slightly better.
```{r}

library("loo")
log_lik_hier <- extract_log_lik(m2.fit, merge_chains = FALSE) 
r_eff <- relative_eff(exp(log_lik_hier),core=2)
loo_hier <- loo(log_lik_hier, r_eff = r_eff,core=2)

print(loo_hier$estimates)

log_lik_pool <- extract_log_lik(pool.fit, merge_chains = FALSE) 
r_eff <- relative_eff(exp(log_lik_pool),core=2)
loo_pool <- loo(log_lik_pool, r_eff = r_eff,core=2)

print(loo_pool$estimates)

comp <- loo_compare(loo_hier,loo_pool)
print(comp)

```

# Predictive performance assessment

As shown above, hierarchical model provides accuracy at 0.78 while pooled model's accuracy is 0.75. Accuracy is the most common metric for clsasification task in case of balanced dataset. And it's meaningful in this case.

#  Prior sensitivity analysis

We investigated optional weakly-informative priors on both model. According to rough observation, we believe 'age' and 'anaemia' have positive effect to death_event. Thus, we change the corresponding prior to inverted chi-square distribution. In hierarchical model, hyper prior for $\mu_1$ and $\mu_2$ is now $inv\_chi\_square(1)$The code is shown below.
 
After changing the hyprior, accuracy decreased to 0.75.

```{stan output.var="hier_2"}
// hierarchical 

data {
  int<lower=0> N;            // number of observations
  int<lower=1> M;            // number of data groups
  int<lower=1> K;            // number of predictors
  int<lower=0,upper=1> y[N]; // outcome (binary)
  int<lower=1,upper=M> g[N]; // map observations to groups
  row_vector[K] x[N];        // predictors
  // test data below
  int<lower=0> N_test;
  int<lower=1,upper=M> g_test[N_test];
  row_vector[K] x_test[N_test];
}
parameters {
  real mu[K];
  real<lower=0> sigma[K];
  vector[K] beta[M];
}
model {
  mu[1:2]~inv_chi_square(1);
  mu[3:8] ~ normal(0, 100);
  for (m in 1:M)
    beta[m] ~ normal(mu, sigma);
  for (n in 1:N)
    y[n] ~ bernoulli_logit(x[n] * beta[g[n]]);
}
generated quantities {
  real y_test[N_test];
  for (n in 1:N_test)
    y_test[n] = bernoulli_logit_rng(x_test[n] * beta[g_test[n]]);
}
```

```{r,message=FALSE,results='hide'}

hier_2.data <- list(
  N = nrow(data.train),       # number of observations
  M = 2,                      # number of groups (male / female)
  K = ncol(data.train) - 4,   # all but outcome + group (= sex)
  y = data.train$DEATH_EVENT,
  g = data.train$sex + 1,
  x = subset(data.train, select=-c(DEATH_EVENT, sex,platelets,creatinine_phosphokinase)),
  # test data
  N_test = nrow(data.test),
  g_test = data.test$sex + 1,
  x_test = subset(data.test, select=-c(DEATH_EVENT, sex,platelets,creatinine_phosphokinase))
)

hier_2.fit <- rstan::sampling(hier_2, data=hier_2.data, seed=SEED)
hier_2.draws <- extract(hier_2.fit)
hier_2.preds <- as.integer((hier_2.draws$y_test %>% colMeans) >= 0.5)
hier_2.accuracy <- mean(hier_2.preds == data.test$DEATH_EVENT)
print(hier_2.accuracy)
```

The prior of $\beta_1$ and $\beta_2$ is also $inv\_chi\_square(1)$. The accyracy is still 0.75.
```{stan output.var="pool"}
data {
  int<lower=0> N;            // number of observations
  int<lower=1> K;            // number of predictors
  int<lower=0,upper=1> y[N]; // outcome (binary)
  row_vector[K] x[N];        // predictors
  // test data below
  int<lower=0> N_test;
  row_vector[K] x_test[N_test];
}
parameters {
  vector[K] beta;
}
model {
  beta[1:2]~inv_chi_square(1);
  beta[3:8] ~ normal(0, 10);
  for (n in 1:N)
    y[n] ~ bernoulli_logit(x[n] * beta);
}
generated quantities {
  real y_test[N_test];
    
  for (n in 1:N_test)
    y_test[n] = bernoulli_logit_rng(x_test[n] * beta);
}
```

```{r,message=FALSE,results='hide'}
#selected features
pool.data <- list(
  N = nrow(data.train),       # number of observations
  K = ncol(data.train) - 4,   # all but outcome + group (= sex)
  y = data.train$DEATH_EVENT,
  x = subset(data.train, select=-c(DEATH_EVENT, sex,platelets,creatinine_phosphokinase)),
  # test data
  N_test = nrow(data.test),
  x_test = subset(data.test,select=-c(DEATH_EVENT,sex,platelets,creatinine_phosphokinase))
)

pool_2.fit <- rstan::sampling(pool, data=pool.data, seed=SEED)
pool_2.draws <- extract(pool_2.fit)
pool_2.preds <- as.integer((pool_2.draws$y_test %>% colMeans) >= 0.5)
pool_2.accuracy <- mean(pool_2.preds == data.test$DEATH_EVENT)
print(pool_2.accuracy)
```

# Problems and solution

As shown above, hierarchical model with selected feature set could give higher accuracy, which is 0.78. However, hierarchical model shows unstable performance during prior sensitivity check, and relatively lower elpd_loo. Pooled model is more stable but provides lower accuracy which is 0.75.

Potential solution could be: getting more data and discretize numerical data to group data in a more micro level. So that each group could have enough data to build binomial model.

# Conclusion

In this project, heart disease data is predicted using hierarchical model and pooled model with weakly-informative prior. Feature selection is conducted and increases the accuracy from 0.73 to 0.78 for hierarchical model. Both model is reliable according to convergence analysis and posterior check. While hierarchical model shows higher accuracy, pooled model has higher PSIS-elpd and is more stable in case of different priors. In conclusion, two models are comparable.

# Self reflection

In this project, we reviewed the knowledge of this course and connected it together. Finding data by ourselves allows us to analyze the data better. And we realize that there is no perfect model, and the comparison of models needs to be done at many levels.


# References
- https://mc-stan.org/docs/2_18/stan-users-guide/hierarchical-logistic-regression.html
- https://archive.ics.uci.edu/ml/datasets/Heart+failure+clinical+records

